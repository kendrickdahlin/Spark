{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aae8e165",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best centroids found by PSO:\n",
      "[[45.66153877 53.13936479]\n",
      " [45.72495507 49.08068079]]\n",
      "Execution time: 95.2610650062561 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from pyspark.sql import SparkSession # type: ignore\n",
    "from pyspark.sql.functions import col # type: ignore\n",
    "\n",
    "Spark = SparkSession.builder.appName('SCPSO').getOrCreate()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Read the dataset from the CSV file\n",
    "data = Spark.read.csv('ScpsoDataset.csv', header=True, inferSchema=True)\n",
    "#data.show()  # Displaying the DataFrame\n",
    "\n",
    "\n",
    "# PSO parameters\n",
    "num_particles = 20\n",
    "max_iterations = 10\n",
    "inertia_weight = 0.7\n",
    "cognitive_coefficient = 1.5\n",
    "social_coefficient = 1.5\n",
    "num_classes = data.select('label').distinct().count()  # Number of unique classes\n",
    "\n",
    "\n",
    "\n",
    "# Initialize particles with random centroids\n",
    "def initialize_particles():\n",
    "    particles = []\n",
    "    for _ in range(num_particles):\n",
    "        centroids = np.random.rand(num_classes, 2) * 100  # Random initial centroids, scaled to match data range\n",
    "        velocity = np.zeros((num_classes, 2))  # Initialize velocity to zeros\n",
    "        particles.append({'centroids': centroids, 'velocity': velocity, 'pbest_centroids': centroids.copy(), 'pbest_fitness': np.inf})\n",
    "    return particles\n",
    "\n",
    "# Fitness function F1\n",
    "def fitness_F1(centroids):\n",
    "    distances = []\n",
    "    for row in data.collect():\n",
    "        x = row['x']\n",
    "        y = row['y']\n",
    "        label = row['label']\n",
    "        centroid = centroids[ord(label) - ord('A')]  # Convert label to index\n",
    "        distances.append(np.linalg.norm([x - centroid[0], y - centroid[1]]))\n",
    "    return np.mean(distances)\n",
    "\n",
    "# Fitness function Fpsi\n",
    "def fitness_Fpsi(centroids):\n",
    "    incorrect_count = data.rdd.map(lambda row: (row['x'], row['y'], row['label'])).map(lambda row: (row[0], row[1], row[2], np.argmin(np.linalg.norm(centroids - [row[0], row[1]], axis=1)))).filter(lambda x: x[3] != ord(x[2]) - ord('A')).count()\n",
    "    return incorrect_count / data.count()  # Percentage of incorrectly classified instances\n",
    "\n",
    "# Fitness function F2\n",
    "def fitness_F2(centroids):\n",
    "    f1 = fitness_F1(centroids)\n",
    "    fpsi = fitness_Fpsi(centroids)\n",
    "    return (f1 + fpsi) / 2\n",
    "\n",
    "# PSO update equations\n",
    "def update_velocity_position(particle, gbest_centroids):\n",
    "    global inertia_weight, cognitive_coefficient, social_coefficient\n",
    "    r1 = np.random.rand()\n",
    "    r2 = np.random.rand()\n",
    "    particle['velocity'] = (inertia_weight * particle['velocity'] +\n",
    "                            cognitive_coefficient * r1 * (particle['pbest_centroids'] - particle['centroids']) +\n",
    "                            social_coefficient * r2 * (gbest_centroids - particle['centroids']))\n",
    "    particle['centroids'] += particle['velocity']\n",
    "\n",
    "# Main PSO function\n",
    "def pso():\n",
    "    particles = initialize_particles()\n",
    "    gbest_centroids = None\n",
    "    gbest_fitness = np.inf\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        for particle in particles:\n",
    "            fitness = fitness_F2(particle['centroids'])  # Use F2 as the fitness function\n",
    "            if fitness < particle['pbest_fitness']:\n",
    "                particle['pbest_fitness'] = fitness\n",
    "                particle['pbest_centroids'] = particle['centroids'].copy()\n",
    "            if fitness < gbest_fitness:\n",
    "                gbest_fitness = fitness\n",
    "                gbest_centroids = particle['centroids'].copy()\n",
    "        for particle in particles:\n",
    "            update_velocity_position(particle, gbest_centroids)\n",
    "\n",
    "    return gbest_centroids\n",
    "\n",
    "# Example usage\n",
    "\n",
    "best_centroids = pso()\n",
    "print(\"Best centroids found by PSO:\")\n",
    "print(best_centroids)\n",
    "end_time = time.time()\n",
    "\n",
    "# Step 3: Calculate the duration\n",
    "duration = end_time - start_time\n",
    "\n",
    "# Step 4: Print the duration\n",
    "print(\"Execution time:\", duration, \"seconds\")\n",
    "\n",
    "Spark.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c79508a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
