Spark Executor Command: "/Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home/bin/java" "-cp" "/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/conf/:/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/jars/*" "-Xmx1024M" "-Dspark.driver.port=61651" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@dyn230-001.wireless-1725.ndsu.nodak.edu:61651" "--executor-id" "0" "--hostname" "172.25.230.1" "--cores" "2" "--app-id" "app-20240508160119-0017" "--worker-url" "spark://Worker@172.25.230.1:59674" "--resourceProfileId" "0"
========================================

Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
24/05/08 16:01:22 INFO CoarseGrainedExecutorBackend: Started daemon with process name: 27577@dyn230-001.wireless-1725.ndsu.NoDak.edu
24/05/08 16:01:22 INFO SignalUtils: Registering signal handler for TERM
24/05/08 16:01:22 INFO SignalUtils: Registering signal handler for HUP
24/05/08 16:01:22 INFO SignalUtils: Registering signal handler for INT
24/05/08 16:01:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/05/08 16:01:24 INFO SecurityManager: Changing view acls to: aaronmackenzie
24/05/08 16:01:24 INFO SecurityManager: Changing modify acls to: aaronmackenzie
24/05/08 16:01:24 INFO SecurityManager: Changing view acls groups to: 
24/05/08 16:01:24 INFO SecurityManager: Changing modify acls groups to: 
24/05/08 16:01:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: aaronmackenzie; groups with view permissions: EMPTY; users with modify permissions: aaronmackenzie; groups with modify permissions: EMPTY
24/05/08 16:01:24 INFO TransportClientFactory: Successfully created connection to dyn230-001.wireless-1725.ndsu.NoDak.edu/172.25.230.1:61651 after 256 ms (0 ms spent in bootstraps)
24/05/08 16:01:25 INFO SecurityManager: Changing view acls to: aaronmackenzie
24/05/08 16:01:25 INFO SecurityManager: Changing modify acls to: aaronmackenzie
24/05/08 16:01:25 INFO SecurityManager: Changing view acls groups to: 
24/05/08 16:01:25 INFO SecurityManager: Changing modify acls groups to: 
24/05/08 16:01:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: aaronmackenzie; groups with view permissions: EMPTY; users with modify permissions: aaronmackenzie; groups with modify permissions: EMPTY
24/05/08 16:01:25 INFO TransportClientFactory: Successfully created connection to dyn230-001.wireless-1725.ndsu.NoDak.edu/172.25.230.1:61651 after 15 ms (0 ms spent in bootstraps)
24/05/08 16:01:25 INFO DiskBlockManager: Created local directory at /private/var/folders/rh/d6ltsplj4f94dtzy8y9sz6lr0000gn/T/spark-015f1d75-d80c-42c8-95d8-a26144992d9f/executor-46b8d1fb-8e39-4c30-a470-1ba400049887/blockmgr-2d5ae080-8f76-4c57-811f-2d799188d6c9
24/05/08 16:01:25 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
24/05/08 16:01:26 INFO CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@dyn230-001.wireless-1725.ndsu.nodak.edu:61651
24/05/08 16:01:26 INFO WorkerWatcher: Connecting to worker spark://Worker@172.25.230.1:59674
24/05/08 16:01:26 INFO TransportClientFactory: Successfully created connection to /172.25.230.1:59674 after 78 ms (0 ms spent in bootstraps)
24/05/08 16:01:26 INFO ResourceUtils: ==============================================================
24/05/08 16:01:26 INFO ResourceUtils: No custom resources configured for spark.executor.
24/05/08 16:01:26 INFO ResourceUtils: ==============================================================
24/05/08 16:01:26 INFO WorkerWatcher: Successfully connected to spark://Worker@172.25.230.1:59674
24/05/08 16:01:27 INFO CoarseGrainedExecutorBackend: Successfully registered with driver
24/05/08 16:01:27 INFO Executor: Starting executor ID 0 on host 172.25.230.1
24/05/08 16:01:27 INFO Executor: OS info Mac OS X, 14.4.1, x86_64
24/05/08 16:01:27 INFO Executor: Java version 1.8.0_401
24/05/08 16:01:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61671.
24/05/08 16:01:27 INFO NettyBlockTransferService: Server created on 172.25.230.1:61671
24/05/08 16:01:27 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/05/08 16:01:27 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(0, 172.25.230.1, 61671, None)
24/05/08 16:01:27 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(0, 172.25.230.1, 61671, None)
24/05/08 16:01:27 INFO BlockManager: Initialized BlockManager: BlockManagerId(0, 172.25.230.1, 61671, None)
24/05/08 16:01:27 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/05/08 16:01:27 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@214b382d for default.
24/05/08 16:01:35 INFO CoarseGrainedExecutorBackend: Got assigned task 2
24/05/08 16:01:35 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
24/05/08 16:01:35 INFO TorrentBroadcast: Started reading broadcast variable 5 with 1 pieces (estimated total size 4.0 MiB)
24/05/08 16:01:35 INFO TransportClientFactory: Successfully created connection to dyn230-001.wireless-1725.ndsu.NoDak.edu/172.25.230.1:61654 after 4 ms (0 ms spent in bootstraps)
24/05/08 16:01:35 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 366.3 MiB)
24/05/08 16:01:35 INFO TorrentBroadcast: Reading broadcast variable 5 took 117 ms
24/05/08 16:01:35 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 11.9 KiB, free 366.3 MiB)
24/05/08 16:01:36 INFO FileScanRDD: Reading File path: file:///opt/homebrew/Cellar/apache-spark/3.5.0/libexec/4Cluster2Ddataset.csv, range: 0-19230, partition values: [empty row]
24/05/08 16:01:37 INFO CodeGenerator: Code generated in 324.797 ms
24/05/08 16:01:37 INFO TorrentBroadcast: Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)
24/05/08 16:01:37 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 366.2 MiB)
24/05/08 16:01:37 INFO TorrentBroadcast: Reading broadcast variable 4 took 28 ms
24/05/08 16:01:37 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 519.0 KiB, free 365.7 MiB)
24/05/08 16:01:37 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 11631 bytes result sent to driver
24/05/08 16:01:38 INFO CoarseGrainedExecutorBackend: Got assigned task 3
24/05/08 16:01:38 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
24/05/08 16:01:38 INFO CoarseGrainedExecutorBackend: Got assigned task 7
24/05/08 16:01:38 INFO Executor: Running task 4.0 in stage 3.0 (TID 7)
24/05/08 16:01:38 INFO TorrentBroadcast: Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
24/05/08 16:01:38 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 365.7 MiB)
24/05/08 16:01:38 INFO TorrentBroadcast: Reading broadcast variable 6 took 20 ms
24/05/08 16:01:38 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 11.7 KiB, free 365.7 MiB)
0 [77.04582961981588, 36.997562146507704]
0 [78.86229342591488, 23.606593954348604]
1 [79.27434810738207, 36.720594811428676]
1 [77.86013373012752, 24.55200799983423]
2 [72.49379211603893, 32.737740377350384]
2 [66.01313391627369, 28.026786180174767]
3 [68.33027254876257, 37.112865020961536]
3 [64.4725947824098, 26.15986390825799]
4 [72.23920752978488, 29.253913946275645]
4 [71.39271321267748, 16.560843978102124]
5 [72.21212560071686, 26.079859584141264]
5 [64.57797341125081, 25.407990171049363]
6 [73.13841481743951, 33.32449021358505]
6 [71.33837992914019, 26.17544317358823]
7 [72.32484133643334, 27.098834794259098]
7 [76.035959963913, 24.112624838884802]
8 [68.36701059117125, 35.06373944493816]
8 [77.35630148544321, 28.582884393181413]
9 [66.94271176325624, 33.34204486584866]
Centroid found by core for label A: [66.94271176325624, 33.34204486584866]
9 [66.94271176325624, 33.34204486584866]
Centroid found by core for label A: [66.94271176325624, 33.34204486584866]
24/05/08 16:01:49 INFO PythonRunner: Times: total = 10969, boot = 561, init = 249, finish = 10159
24/05/08 16:01:49 INFO PythonRunner: Times: total = 10969, boot = 575, init = 235, finish = 10159
24/05/08 16:01:49 INFO Executor: Finished task 4.0 in stage 3.0 (TID 7). 1386 bytes result sent to driver
24/05/08 16:01:49 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1386 bytes result sent to driver
24/05/08 16:01:51 INFO CoarseGrainedExecutorBackend: Got assigned task 13
24/05/08 16:01:51 INFO Executor: Running task 2.0 in stage 4.0 (TID 13)
24/05/08 16:01:51 INFO CoarseGrainedExecutorBackend: Got assigned task 17
24/05/08 16:01:51 INFO Executor: Running task 6.0 in stage 4.0 (TID 17)
24/05/08 16:01:51 INFO TorrentBroadcast: Started reading broadcast variable 7 with 1 pieces (estimated total size 4.0 MiB)
24/05/08 16:01:51 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 365.7 MiB)
24/05/08 16:01:51 INFO TorrentBroadcast: Reading broadcast variable 7 took 73 ms
24/05/08 16:01:51 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 11.7 KiB, free 365.7 MiB)
0 [83.55631727101259, 67.31488849992068]
0 [76.09316233946095, 68.66058691751222]
1 [67.64149306134937, 77.58919756896331]
24/05/08 16:01:53 ERROR Executor: Exception in task 2.0 in stage 4.0 (TID 13)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 1247, in main
    process()
  File "/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 1237, in process
    out_iter = func(split_index, iterator)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/aaronmackenzie/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/rdd.py", line 5434, in pipeline_func
  File "/Users/aaronmackenzie/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/rdd.py", line 840, in func
  File "/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/a.py", line 118, in <lambda>
    centroid_B = dataset_rdd.filter(lambda row: row['Class'] == 'B').mapPartitions(lambda partition: [PSO(partition, 'B')]).collect()
                                                                                                      ^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/a.py", line 65, in PSO
    prev_gbest = gbest_position.copy()
                 ^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'copy'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:366)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1046)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
24/05/08 16:01:53 INFO CoarseGrainedExecutorBackend: Got assigned task 19
24/05/08 16:01:53 INFO Executor: Running task 2.1 in stage 4.0 (TID 19)
2 [80.09124332608627, 74.20813851533342]
0 [65.19109549063793, 71.64563657649202]
3 [65.19109549063793, 71.64563657649202]
1 [65.19109549063793, 71.64563657649202]
4 [65.19109549063793, 71.64563657649202]
2 [67.57631045100854, 75.91106118808646]
5 [77.5426650442867, 64.53468610208424]
3 [65.52870448544569, 74.19192864132211]
6 [65.52870448544569, 74.19192864132211]
4 [65.52870448544569, 74.19192864132211]
7 [67.09210126961688, 75.23137863171381]
5 [70.95549813632385, 73.78379391900211]
8 [74.2265042805749, 73.1247804962124]
6 [70.95549813632385, 73.78379391900211]
9 [70.95549813632385, 73.78379391900211]
Centroid found by core for label B: [70.95549813632385, 73.78379391900211]
24/05/08 16:02:01 INFO PythonRunner: Times: total = 10370, boot = -1997, init = 2095, finish = 10272
24/05/08 16:02:01 INFO Executor: Finished task 6.0 in stage 4.0 (TID 17). 1386 bytes result sent to driver
7 [70.95549813632385, 73.78379391900211]
8 [70.08099019149091, 72.51610288700712]
9 [67.83373273592028, 72.50226661782182]
Centroid found by core for label B: [67.83373273592028, 72.50226661782182]
24/05/08 16:02:03 INFO PythonRunner: Times: total = 10254, boot = 7, init = 64, finish = 10183
24/05/08 16:02:03 INFO Executor: Finished task 2.1 in stage 4.0 (TID 19). 1386 bytes result sent to driver
24/05/08 16:02:04 INFO CoarseGrainedExecutorBackend: Got assigned task 20
24/05/08 16:02:04 INFO CoarseGrainedExecutorBackend: Got assigned task 24
24/05/08 16:02:04 INFO Executor: Running task 4.0 in stage 5.0 (TID 24)
24/05/08 16:02:04 INFO Executor: Running task 0.0 in stage 5.0 (TID 20)
24/05/08 16:02:04 INFO TorrentBroadcast: Started reading broadcast variable 8 with 1 pieces (estimated total size 4.0 MiB)
24/05/08 16:02:04 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 365.7 MiB)
24/05/08 16:02:04 INFO TorrentBroadcast: Reading broadcast variable 8 took 54 ms
24/05/08 16:02:04 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 11.7 KiB, free 365.7 MiB)
0 [31.56311265633446, 30.073714985282507]
0 [34.14882516549087, 39.27818869320605]
1 [34.575644904206534, 33.60058221661198]
1 [34.64781510623615, 37.87313269764125]
2 [37.523444942135235, 28.15340110311371]
2 [28.605371198729365, 33.57776433711281]
3 [28.605371198729365, 33.57776433711281]
3 [28.605371198729365, 33.57776433711281]
4 [28.605371198729365, 33.57776433711281]
4 [30.569572674242373, 34.82810730369089]
5 [32.404680706632234, 34.545356037364506]
5 [34.933561993815786, 37.8697526713231]
6 [34.27327182321706, 36.36814602530088]
6 [34.27327182321706, 36.36814602530088]
7 [34.93190480240551, 36.649128302442136]
7 [27.60788504007533, 28.09435549632325]
8 [27.25678078785419, 25.52866366137695]
8 [38.55144166715698, 32.20491990853714]
9 [25.771233641352666, 21.96511643076181]
Centroid found by core for label C: [25.771233641352666, 21.96511643076181]
9 [25.771233641352666, 21.96511643076181]
Centroid found by core for label C: [25.771233641352666, 21.96511643076181]
24/05/08 16:02:14 INFO PythonRunner: Times: total = 10315, boot = -2466, init = 2560, finish = 10221
24/05/08 16:02:14 INFO PythonRunner: Times: total = 10316, boot = -297, init = 401, finish = 10212
24/05/08 16:02:14 INFO Executor: Finished task 0.0 in stage 5.0 (TID 20). 1386 bytes result sent to driver
24/05/08 16:02:14 INFO Executor: Finished task 4.0 in stage 5.0 (TID 24). 1386 bytes result sent to driver
24/05/08 16:02:14 INFO CoarseGrainedExecutorBackend: Got assigned task 29
24/05/08 16:02:14 INFO CoarseGrainedExecutorBackend: Got assigned task 33
24/05/08 16:02:14 INFO Executor: Running task 1.0 in stage 6.0 (TID 29)
24/05/08 16:02:14 INFO Executor: Running task 5.0 in stage 6.0 (TID 33)
24/05/08 16:02:14 INFO TorrentBroadcast: Started reading broadcast variable 9 with 1 pieces (estimated total size 4.0 MiB)
24/05/08 16:02:14 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 365.7 MiB)
24/05/08 16:02:14 INFO TorrentBroadcast: Reading broadcast variable 9 took 55 ms
24/05/08 16:02:14 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 11.7 KiB, free 365.7 MiB)
0 [28.973937106723554, 43.52638459767777]
0 [31.816988786770043, 68.51598746624292]
1 [32.31316594449156, 64.41065652989778]
1 [37.40064451327285, 67.46992627031382]
2 [33.85391579863199, 70.7646715445399]
2 [33.85391579863199, 70.7646715445399]
3 [30.1358535292911, 72.10600138710613]
3 [27.27979305497343, 65.85914684087314]
4 [27.27979305497343, 65.85914684087314]
4 [24.50875194647292, 70.69862535317019]
5 [23.851561888798805, 56.42446147161225]
5 [36.62961997171607, 71.1458470473817]
6 [30.540668895868478, 69.34156894734943]
6 [29.895133010493495, 67.83968870605939]
7 [34.98979926177789, 66.95206896070086]
7 [34.98979926177789, 66.95206896070086]
8 [30.701356482098475, 63.629001936884606]
8 [30.701356482098475, 63.629001936884606]
9 [30.701356482098475, 63.629001936884606]
Centroid found by core for label D: [30.701356482098475, 63.629001936884606]
24/05/08 16:02:25 INFO PythonRunner: Times: total = 10293, boot = -303, init = 361, finish = 10235
24/05/08 16:02:25 ERROR Executor: Exception in task 5.0 in stage 6.0 (TID 33)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 1247, in main
    process()
  File "/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/pyspark.zip/pyspark/worker.py", line 1237, in process
    out_iter = func(split_index, iterator)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/aaronmackenzie/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/rdd.py", line 5434, in pipeline_func
  File "/Users/aaronmackenzie/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/rdd.py", line 840, in func
  File "/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/a.py", line 124, in <lambda>
    centroid_D = dataset_rdd.filter(lambda row: row['Class'] == 'D').mapPartitions(lambda partition: [PSO(partition, 'D')]).collect()
                                                                                                      ^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/a.py", line 65, in PSO
    prev_gbest = gbest_position.copy()
                 ^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'copy'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:366)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1046)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
24/05/08 16:02:25 INFO Executor: Finished task 1.0 in stage 6.0 (TID 29). 1386 bytes result sent to driver
24/05/08 16:02:25 INFO CoarseGrainedExecutorBackend: Got assigned task 36
24/05/08 16:02:25 INFO Executor: Running task 5.1 in stage 6.0 (TID 36)
0 [32.10079779566213, 70.49852375798227]
1 [32.10079779566213, 70.49852375798227]
2 [32.10079779566213, 70.49852375798227]
3 [34.01332140905206, 73.45381430191006]
4 [34.01332140905206, 73.45381430191006]
5 [34.01332140905206, 73.45381430191006]
6 [32.242442221200115, 72.87351001852602]
7 [32.242442221200115, 72.87351001852602]
8 [32.242442221200115, 72.87351001852602]
9 [33.506790542762054, 72.08756389989038]
Centroid found by core for label D: [33.506790542762054, 72.08756389989038]
24/05/08 16:02:35 INFO PythonRunner: Times: total = 10215, boot = -131, init = 169, finish = 10177
24/05/08 16:02:35 INFO Executor: Finished task 5.1 in stage 6.0 (TID 36). 1386 bytes result sent to driver
24/05/08 16:02:35 INFO CoarseGrainedExecutorBackend: Driver commanded a shutdown
