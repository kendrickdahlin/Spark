{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d9244f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/07 10:46:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/06/07 10:46:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/06/07 10:46:25 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centroid for class 1: [21.69662136081831, 22.973663088219773]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centroid for class 2: [5.901040022245917, 25.765043830059554]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centroid for class 3: [31.919936351374446, 8.194478287625278]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centroid for class 4: [22.351439715833358, 8.744826238799408]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centroid for class 5: [17.32199404796215, 6.620641542537554]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centroid for class 6: [33.66064097219253, 24.09394676394267]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centroid for class 7: [13.96513889872401, 14.142789348454412]\n",
      "Accuracy:  0.7088607594936709\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "##SPLITS PARTICLES INTO PARTITIONS\n",
    "class FireflyAlgorithm:\n",
    "    def __init__(self, n_fireflies=56, max_iter=20, alpha=0.3, beta0=1, gamma=0.04):\n",
    "        self.n_fireflies = n_fireflies\n",
    "        self.max_iter = max_iter\n",
    "        self.alpha = alpha\n",
    "        self.beta0 = beta0\n",
    "        self.gamma = gamma\n",
    "        self.lb = 0 \n",
    "        self.ub = 100\n",
    "        self.centroids = {}\n",
    "        self.points = []\n",
    "\n",
    "    def objective_function(self, x):\n",
    "        return np.sum(np.linalg.norm(np.subtract(self.points,x), axis = 1))\n",
    "\n",
    "    def find_center(self, points):\n",
    "        #clean points data\n",
    "        self.points = [list(i) for i in list(points)]\n",
    "        dim = len(self.points[0])\n",
    "        \n",
    "        #initialize fireflies\n",
    "        fireflies = np.random.uniform(self.lb, self.ub, (self.n_fireflies, dim))\n",
    "        fitness = np.apply_along_axis(self.objective_function, 1, fireflies)\n",
    "        \n",
    "        \n",
    "        #set arbitrary global best\n",
    "        best_firefly = fireflies[0]\n",
    "        best_fitness = fitness[0]\n",
    "        \n",
    "        for k in range(self.max_iter):\n",
    "            k_alpha = self.alpha * (1-k/self.max_iter) # decreases alpha over time\n",
    "           \n",
    "            for i in range(self.n_fireflies):\n",
    "                for j in range(self.n_fireflies):\n",
    "                    ##Here check broadcast variable\n",
    "                    if fitness[j] < fitness[i]:\n",
    "                        #move firefly\n",
    "                        r = np.linalg.norm(np.subtract(fireflies[i], fireflies[j])) #distance\n",
    "                        beta = self.beta0 * np.exp(-self.gamma * r**2) #attractiveness\n",
    "                        random_factor = k_alpha * (np.random.rand(dim) - 0.5) #randomness\n",
    "                        #moves firefly based on equation \n",
    "                        fireflies[i] += beta * (np.subtract(fireflies[j],fireflies[i])) + random_factor\n",
    "                        fireflies[i] = np.clip(fireflies[i], self.lb, self.ub) # keeps new loc within range\n",
    "\n",
    "                        #update fitness\n",
    "                        fitness[i] = self.objective_function(fireflies[i])\n",
    "                        #update new best\n",
    "                    \n",
    "                        if fitness[i] < best_fitness:\n",
    "                            #update global best\n",
    "                            best_firefly = fireflies[i]\n",
    "                            best_fitness = fitness[i]\n",
    "        return best_firefly\n",
    "    \n",
    "    #returns string of classification\n",
    "    def classify(self, point):\n",
    "        distances = {}\n",
    "        for cls, centroid in self.centroids.items():\n",
    "            distances[cls]= np.linalg.norm(np.subtract(centroid,point))\n",
    "        cls = min(distances, key = distances.get)\n",
    "        return cls\n",
    "    \n",
    "\n",
    "    def run(self, file_name):\n",
    "        # Create a SparkSession\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"Firefly Algorithm with Spark\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "        sc = spark.sparkContext\n",
    "        num_cores = sc.defaultParallelism  #Determine the number of available cores\n",
    "        self.n_fireflies = max(self.n_fireflies, num_cores) \n",
    "        \n",
    "\n",
    "        # Read the dataset from CSV file into a  DataFrame\n",
    "        df = pd.read_csv(file_name)\n",
    "        \n",
    "        df = df.sample(frac=1) # shuffle df\n",
    "        ratio = 0.8\n",
    " \n",
    "        total_rows = df.shape[0]\n",
    "        train_size = int(total_rows*ratio)\n",
    "        \n",
    "        # Split data into test and train\n",
    "        train_df = df[0:train_size]\n",
    "        test_df = df[train_size:]\n",
    "\n",
    "        #train\n",
    "        feature_columns = train_df.columns[:-1]\n",
    "        class_column = train_df.columns[-1]\n",
    "        classes = train_df[class_column].unique()\n",
    "        classes.sort()\n",
    "        \n",
    "        \n",
    "        for cls in classes:\n",
    "            points = train_df[train_df[class_column] == cls][feature_columns].values\n",
    "            points_rdd = sc.parallelize(points)\n",
    "            center = points_rdd.mapPartitions(lambda points: [self.find_center(points)]).collect()\n",
    "            #clean appearance\n",
    "            center = list(map(lambda point: list(point), center))\n",
    "            \n",
    "            self.centroids[cls] = [sum(x) / len(center) for x in zip(*center)]\n",
    "            print(f\"Centroid for class {cls}: {self.centroids[cls]}\")\n",
    "                \n",
    "    \n",
    "        #test\n",
    "        accuracy = 0\n",
    "        count = 0\n",
    "        for index, row in test_df.iterrows():\n",
    "            cls = self.classify(row[:-1].values)\n",
    "            if cls == row[-1]:\n",
    "                accuracy +=1\n",
    "            count +=1\n",
    "        print(\"Accuracy: \", accuracy/count)\n",
    "        # Stop the SparkSession\n",
    "        spark.stop()\n",
    "        return self.centroids\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fa = FireflyAlgorithm()\n",
    "    fa.run(\"Data/Aggregation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c429ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
