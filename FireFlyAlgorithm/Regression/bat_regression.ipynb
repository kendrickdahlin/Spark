{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3436a22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/21 11:33:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/06/21 11:34:19 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 4:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "\n",
    "def objective_function(model, X, y):\n",
    "    pred = (np.dot(X, model[:-1]) + model[-1] >= 0).astype(int)\n",
    "    mse = np.mean(np.subtract(y, pred) ** 2)\n",
    "    return mse\n",
    "\n",
    "def bat(X, y):\n",
    "    #set params\n",
    "    dim = X.shape[1]+1\n",
    "    num_bats = 70\n",
    "    num_gens = 100\n",
    "    Lbound = -5\n",
    "    Ubound = 5\n",
    "    Qmin = 0\n",
    "    Qmax = (Ubound-Lbound)/num_bats\n",
    "    pulse_rate = 0.1\n",
    "    loudness=0.9\n",
    "    \n",
    "    positions = np.random.uniform(Lbound, Ubound, (num_bats, dim))\n",
    "    velocities = np.zeros((num_bats, dim))\n",
    "    fitness = np.apply_along_axis(objective_function, 1, positions, X, y)\n",
    "    \n",
    "    gbest_position = positions[np.argmin(fitness)]\n",
    "    gbest_fitness = np.min(fitness)\n",
    "\n",
    "    for iteration in range(num_gens):\n",
    "        for i in range(num_bats):\n",
    "            #generate new solutions by adjusting frequency\n",
    "            freq = Qmin + (Qmax - Qmin) * np.random.rand()\n",
    "            velocities[i] += (positions[i] - gbest_position) * freq\n",
    "            new_position = positions[i] + velocities[i]\n",
    "\n",
    "            \n",
    "            if np.random.rand() < pulse_rate:\n",
    "                #generate a location solution around best solution\n",
    "                new_position = gbest_position + 0.001 * np.random.randn(dim)\n",
    "             \n",
    "            new_fitness = objective_function(new_position, X, y)\n",
    "            \n",
    "            #if new fitness < fitness[i] and rand < loudness\n",
    "            if new_fitness < fitness[i] and np.random.rand() < loudness:\n",
    "                #accept new solutions\n",
    "                positions[i] = new_position\n",
    "                fitness[i] = new_fitness\n",
    "            #update new global bests\n",
    "            if new_fitness < gbest_fitness:\n",
    "                gbest_position = new_position.copy()\n",
    "                gbest_fitness = new_fitness\n",
    "\n",
    "    return gbest_position\n",
    "\n",
    "def predict(model, X):\n",
    "    pred = (np.dot(X, model[:-1]) + model[-1] >= 0).astype(int)\n",
    "    return pred\n",
    "\n",
    "def run(file_name):\n",
    "    spark = SparkSession.builder \\\n",
    "            .appName(\"Bat Algorithm with Spark\") \\\n",
    "            .getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    #read data\n",
    "    df = spark.read.csv(file_name, header=True, inferSchema=True)\n",
    "    X = np.array(df.select(df.columns[:-1]).collect())\n",
    "    y = np.array(df.select(df.columns[-1]).collect()).flatten()\n",
    "   \n",
    "    y = LabelEncoder().fit_transform(y)  #transform y values to ints\n",
    "    X = StandardScaler().fit_transform(X) #scale X values\n",
    "   \n",
    " \n",
    "    #Create an RDD of (feature, label) pairs\n",
    "    data_rdd = sc.parallelize(list(zip(X, y)))\n",
    "\n",
    "    #Firefly algorithm applied to partitions\n",
    "    def bat_partition(partition):\n",
    "        partition_list = list(partition)\n",
    "        if len(partition_list) == 0:\n",
    "            return []\n",
    "        X_partition, y_partition = zip(*partition_list)\n",
    "        X_partition = np.array(X_partition)\n",
    "        y_partition = np.array(y_partition)\n",
    "        return [bat(X_partition, y_partition)]\n",
    "\n",
    "    # Apply Firefly algorithm to each partition and collect results\n",
    "    weights = data_rdd.mapPartitions(bat_partition).collect()\n",
    "    model = [sum(x) / len(weights) for x in zip(*weights)]\n",
    "    \n",
    "    y_pred = predict(model,X)\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    mse = np.mean(np.subtract(y,y_pred)**2)\n",
    "    print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "    \n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run('Behavior.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2b1b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
